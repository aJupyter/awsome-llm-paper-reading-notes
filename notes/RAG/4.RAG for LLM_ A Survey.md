[RAG_Slide_ENG.pdf](https://www.yuque.com/attachments/yuque/0/2024/pdf/12752365/1709109907414-9b22ec82-ed40-41e3-b9a5-f74e19697638.pdf)

[喂饭！RAG for LLM: A Survey论文导读](https://zhuanlan.zhihu.com/p/673835082)

[两万字RAG论文综述--大语言模型（LLMs）的检索增强生成](https://zhuanlan.zhihu.com/p/683171274)

[https://arxiv.org/pdf/2312.10997.pdf](https://arxiv.org/pdf/2312.10997.pdf)

[沧海九粟的个人空间-沧海九粟个人主页-哔哩哔哩视频](https://space.bilibili.com/28357052)

# 一、 RAG框架
## 1.Naive RAG
Naive RAG遵循一个传统的流程，包括索引、检索和生成。它也被称为“检索-阅读”框架。

缺点：三个方面：“检索”、“生成”和“增强”

+ 检索质量带来了各种各样的挑战，包括精度低，导致检索块不对齐以及潜在的问题，如幻觉或半空中掉落（mid-air drop）。低回忆率也会发生，导致无法检索所有相关的块，从而阻碍了LLMS全面回应的能力。过时的信息使问题进一步复杂化，可能产生不准确的检索结果。
+ 回复生成质量有幻觉，其中模型生成的答案不基于所提供的上下文，以及模型输出中不相关的上下文和潜在的毒性或偏见问题。
+ 增强过程的挑战在于如何在有效地将检索段落的上下文与当前生成任务集成，中间可能导致不连贯的输出。冗余和重复也是一个问题，特别是当多个检索的段落包含相似的信息时，会导致生成的响应中出现重复的内容。另一个挑战是如何识别多个检索段落对生成任务的重要性和相关性，需要适当平衡每个段落的价值。此外，协调不同的写作风格和语调，以确保输出的一致性是至关重要的。
+ 最后，也存在生成模型过度依赖增强信息的风险，可能导致输出只是重复检索的内容，而没有提供新值或合成信息。

---

## 2.Advanced RAG
Advanced RAG是为了解决Naive RAG的局限性而开发的。在检索质量方面，Advanced RAG实现了预检索和后检索策略。为了解决Naive RAG在索引方面的挑战，Advanced RAG通过滑动窗口、细粒度分割和元数据等技术优化了索引方法。它还引入了各种方法来优化检索过程。

---

### 1.预检索过程（Pre-Retrieval Process）
<font style="color:rgb(25, 27, 31);">优化数据索引：优化数据索引的目标是提高被索引内容的质量。这涉及到五个主要策略：增强数据粒度、优化索引结构、添加元数据、对齐优化和混合检索。</font>

**<font style="color:rgb(25, 27, 31);">增强数据粒度</font>**<font style="color:rgb(25, 27, 31);">旨在提高文本标准化、一致性、事实准确性和丰富上下文，以改善RAG系统的性能。这包括去除不相关信息，消除实体和术语的歧义，确认事实准确性，保持上下文，并更新过时的文档。</font>

**<font style="color:rgb(25, 27, 31);">优化索引结构</font>**<font style="color:rgb(25, 27, 31);">涉及调整块的大小以捕获相关上下文，跨多个索引路径进行查询，并利用图结构中的信息从节点之间的关系中捕获相关上下文。</font>

**<font style="color:rgb(25, 27, 31);">添加元数据信息</font>**<font style="color:rgb(25, 27, 31);">涉及将引用元数据（如日期和目的）集成到块中以进行过滤，并整合元数据（如引用的章节和子部分）以提高检索效率。</font>

**<font style="color:rgb(25, 27, 31);">对齐优化</font>**<font style="color:rgb(25, 27, 31);">通过在文档中引入“假设性query”来解决文档之间的对齐query和差异。</font>

（引入“假设性query”可能是一种策略，用于在文档中寻找潜在的相关信息，即使这些信息不是直接与用户的原始查询完全匹配。这种方法可以帮助系统发现那些可能与用户需求相关但表达方式不同的信息。例如，如果用户询问“苹果的营养价值”，系统可能会引入假设性query，如“苹果的健康益处”或“吃苹果的好处”，以找到更广泛的相关信息。）

---

### <font style="color:rgb(25, 27, 31);">2. 检索过程（Retrieval）</font>
+ 突出 embedding 模型的重要性

<font style="color:rgb(25, 27, 31);">在检索阶段，主要关注的是通过计算query和块之间的相似度来识别合适的上下文。Embedding模型是这个过程的核心。在高级RAG中，有可能对Embedding模型进行优化。</font>

**<font style="color:rgb(25, 27, 31);">微调嵌入。</font>**<font style="color:rgb(25, 27, 31);">微调Embedding模型会显著影响RAG系统中检索内容的相关性。该过程包括自定义Embedding模型，以增强特定领域上下文中的检索相关性，特别是对于处理演化或罕见术语的专业领域。BGE Embedding模型（如BAAI2开发的BGE-large- en），就是一个可以微调以优化检索相关性的高性能Embedding模型的例子。可以使用GPT-3.5-turbo等语言模型生成用于微调的训练数据，以制定基于文档块的query，然后将其用作微调对。</font>

**<font style="color:rgb(25, 27, 31);">动态嵌入。</font>**<font style="color:rgb(25, 27, 31);">动态嵌入可适应单词使用的上下文，不像静态嵌入，它为每个单词使用单个向量[Karpukhin等，2020]。例如，在像BERT这样的变压器模型中，相同的单词可以根据周围的单词具有不同的嵌入。OpenAI的embeddingsada -02模型3建立在llm(如GPT)的原理之上，是一个复杂的动态Embedding模型，可以捕获上下文理解。然而，它可能不会像最新的全尺寸语言模型(如GPT-4)那样对上下文表现出同样的敏感性。</font>

---

### <font style="color:rgb(25, 27, 31);">3. 后检索过程（Post-Retrieval Process）</font>
<font style="color:rgb(25, 27, 31);">在从数据库检索到有价值的上下文后，将其与query作为输入合并到LLMs中时，解决上下文窗口限制带来的挑战至关重要。简单地一次性向LLMs展示所有相关文档可能会超过上下文窗口限制，会引入噪声，并阻碍对关键信息的关注。因此，对检索到的内容进行额外处理是必要的。</font>

**<font style="color:rgb(25, 27, 31);">重新排名。</font>**<font style="color:rgb(25, 27, 31);">重新排名检索到的信息，将最相关的内容重新定位到prompt的边缘，是关键策略。这个概念已经在LlamaIndex4、LangChain5和HayStack6等框架中实现。例如，Diversity Ranker6根据文档多样性优先重新排序，而LostInTheMiddleRanker则在提示的开头和结尾交替放置最佳文档。此外，cohereAI rerank7、bge-rerank8和LongLLMLingua9等方法重新计算相关文本和query之间的语义相似度，解决了基于向量模拟搜索的语义相似度解释的挑战。</font>

**<font style="color:rgb(25, 27, 31);">提示压缩。</font>**<font style="color:rgb(25, 27, 31);">研究表明，检索文档中的噪声对RAG性能有负面影响。在后处理中，重点是压缩不相关的上下文，突出关键段落，并减少整体上下文长度。Selective Context和LLMLingua10等方法利用小型语言模型来计算提示的互信息或困惑度，从而估计模块的重要性。Recomp11通过在不同粒度上训练压缩器来解决这个问题，而Long Context12和“Walking in the Memory Maze”13则设计了总结技术，以增强LLM在处理大量上下文时的关键信息感知。</font>

---

## 3.Modular RAG
Modular RAG结构与传统的Naive RAG框架不同，提供了更大的多样性和灵活性。它整合了各种方法来增强功能模块，例如，整合一个搜索模块进行相似性检索，并在检索器中应用微调方法。重新结构化的RAG模块和迭代方法已经被开发出来以解决特定问题。Modular RAG范式在RAG领域越来越成为常态，允许在多个模块之间进行串行工作流或端到端训练。图3展示了三种RAG范式的比较。然而，Modular RAG并不是独立的。Advanced RAG是Modular RAG的一种专门形式，而Naive RAG本身是Advanced RAG的一个特例。这三种范式之间的关系是继承和发展的关系。

![](https://cdn.nlark.com/yuque/0/2024/png/12752365/1709110953547-3dedd721-07b2-4b29-896d-8d0c40750730.png)

<font style="color:rgb(145, 150, 161);">图3 RAG三种范式的比较</font>

### 新模块
+ 搜索模块（Search Module）。与Naive/Advanced RAG中的相似性检索不同，搜索模块针对特定场景进行了定制，并在额外的语料库上进行直接搜索。这种整合是通过LLM生成的代码实现的，查询语言用的是如SQL或Cypher以及其他定制工具。这些搜索的数据源可以包括搜索引擎、文本数据、表格数据和知识图谱[Wang et al., 2023d]。
+ 记忆模块（Memory Module）。这个模块利用LLM的记忆能力来指导检索。这种方法涉及识别与当前输入最相似的记忆。Selfmem [Cheng et al., 2023b]利用检索增强的生成器迭代地创建一个无界记忆池，结合“原始query”和“双重query”。通过使用检索增强生成模型（使用模型本身的输出来改进自己的模型），可以使文本在推理过程中与数据分布更加一致。因此，利用的是模型的输出被用作训练数据[Wang et al., 2022a]。
+ 融合（Fusion）。RAG-Fusion [Raudaschl, 2023]通过多查询方法增强了传统搜索系统，该方法将用户query扩展为多个query，解决了用户query的局限性。这种多query方法通过扩展用户query，将答案与多个query对齐，从而提高了信息检索的效率和质量。
+ 路由（Routing）。RAG系统的检索过程利用了不同领域、语言和格式的多样化来源，可以根据情况交替或合并[Li et al., 2023b]。query路由器决定用户query的后续操作，选项包括从摘要、搜索特定数据库或将不同路径合并为单个响应。query路由器还会选择query的适当数据存储，可能包括向量存储、图数据库或关系数据库，或多文档存储的索引层次结构，例如，摘要索引和文档块向量索引。query路由器的决策是预定义的，并通过LLM调用执行，将query定向到所选索引。
+ 预测（Predict）。它解决了检索内容中的冗余和噪声问题。与直接从数据源检索不同，这个模块利用LLM生成必要的上下文[Yu et al., 2022]。由LLM生成的内容比通过直接检索获得的内容更有可能包含相关信息。
+ 任务适配器（Task Adapter）。这个模块专注于将RAG适应于各种下游任务。UPRISE自动化了从预构建的数据池中检索零样本任务输入的提示，从而增强了跨任务和模型的通用性[Cheng et al., 2023a]。同时，PROMPTAGA-TOR [Dai et al., 2022]利用LLM作为少量样本query生成器，并根据生成的数据创建任务特定的检索器。通过利用LLM的泛化能力，它能够在最少的示例下开发出特定于任务的端到端检索器。

### 新模式
Modular RAG的组织结构具有高度的适应性，允许在RAG过程中替换或重新排列模块以适应特定query的上下文。

+ 增加或更换模块。引入或替换模块的策略包括维护检索-读取过程的核心结构，同时集成其他模块以增强特定功能。RRR模型[Ma等，2023a]引入了重写-检索-读取过程，利用LLM性能作为重写模块的强化学习激励。这使重写器能够微调检索问题，从而提高读取器的下游任务性能。类似地，在Generate-Read [Yu et al.， 2022]等方法中，模块可以选择性地交换，其中LLM的生成模块取代了检索模块。背诵-阅读（Recite-Read）方法[Sun et al.， 2022]将外部检索转换为基于模型权重的检索，要求LLM首先记住特定于任务的信息，随后产生能够处理知识密集型自然语言处理任务的输出。
+ 调整模块间的流程。在模块流调整领域，重点是加强语言模型和检索模型之间的交互。DSP [Khattab等，2022]引入了演示-搜索-预测框架，将上下文学习系统视为一个明确的程序，而不是最终的任务提示，从而更有效地处理知识密集型任务。ITER-RETGEN [Shao等，2023]方法利用生成的内容来指导检索，在检索-读取-检索-读取流程中迭代地实现“检索增强生成”和“生成增强检索”。这种方法展示了一种使用一个模块的输出来改进另一个模块的功能的创新方法。

---

### **<font style="color:rgb(25, 27, 31);">优化RAG流程</font>**
RAG流程的优化旨在提高RAG系统中信息的效率和质量。当前的研究集中在整合多样化的搜索技术，完善检索步骤，融入认知回溯，实施灵活的query策略，并利用嵌入相似性。这些努力共同致力于在RAG系统中实现检索效率和上下文信息深度之间的平衡。

+ 混合搜索探索。RAG系统通过智能整合各种技术，包括基于关键词的搜索、语义搜索和向量搜索，来优化其性能。这种方法利用每种方法的独特优势，以适应多样化的query类型和信息需求，确保检索到高度相关和内容丰富的信息。
+ 递归检索和问题引擎。递归检索涉及在初始检索阶段获取较小的块以捕获关键语义意义。随后，在过程的后期阶段，向LLM提供包含更多上下文信息的较大块。这种两步检索方法有助于平衡响应效率和提供丰富内容。
+ StepBack-prompt。该方法鼓励LLM远离具体实例，围绕更广泛的概念和原则进行推理[Zheng et al., 2023]。实验结果表明，当使用向后提示时，各种具有挑战性的基于推理的任务的性能显著提高，突显了它们对RAG过程的自然适应性。这些检索增强步骤可以应用于生成向后提示的响应以及最终的问答过程中。
+ 子query。根据场景，可以采用各种query策略，例如使用LlamaIndex提供的query引擎，利用树query，使用向量query，或执行简单的顺序query块。
+ 假设文档嵌入（Hypothetical Document Embeddings，HyDE）。HyDE基于这样一个信念：生成的答案可能比直接查询在嵌入空间中更接近。使用LLM，HyDE为query创建一个假设文档（答案），嵌入这个文档，并使用结果嵌入来检索与假设文档相似的真实文档。这种方法不是基于query的嵌入相似性，而是关注从一个答案到另一个答案的嵌入相似性[Gao et al., 2022]。然而，它可能不会始终产生理想的结果，特别是在语言模型不熟悉主题内容时，可能导致更多错误实例。

---

# 二、检索
在RAG的背景下，从数据源高效检索相关文档至关重要。然而，创建一个熟练的检索器面临着重大挑战。本节将介绍三个基本问题：1）如何实现准确的语义表示？2）哪些方法可以对齐query和文档？3）如何将检索器的输出与大语言模型（LLM）的偏好对齐？

## 1.构建准确的语义空间
### 块优化（Chunk optimization）
在处理外部文档时，最初的步骤涉及将它们分解成较小的块以提取细粒度特征，然后嵌入这些特征以表示它们的语义。然而，嵌入过大或过小的文本块可能会导致较次的优化结果。因此，确定语料库中文档的最佳块大小对于确保检索结果的准确性和相关性至关重要。

选择合适的块策略需要仔细考虑几个重要因素，如索引内容的性质、Embedding模型及其最佳块大小、用户query的预期长度和复杂性，以及应用程序对检索结果的具体应用（例如，语义搜索或问答）。例如，选择块模型应基于内容的长度。此外，不同的Embedding模型在不同的块大小下表现出不同的性能特征。例如，sentence-transformer在单个句子上表现更好，而text-embedding-ada-002在包含256-512个token的块中表现出色。



