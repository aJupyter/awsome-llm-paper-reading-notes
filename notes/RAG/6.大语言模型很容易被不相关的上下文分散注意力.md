大语言模型很容易被不相关的上下文分散注意力

# 洞见

现有的评估方法通常基于与任务高度相关的数据集，这可能无法全面反映LLM在面对真实世界复杂情况时的表现。为此，本研究深入探讨了LLM在处理包含不相关文本的输入时的分散性问题，即LLM在面对干扰信息时的稳定性和准确性。通过引入一个特别设计的小学数学数据集，其中包含了不相关的上下文信息，本研究对LLM的抗干扰能力进行了系统的测试。结果表明，尽管LLM在许多任务上表现出色，但它们在处理这类含有噪声的输入时确实容易受到干扰。为了解决这一问题，本研究提出了一系列创新的方法，包括自我一致性解码和添加特定的Prompt指令，以引导模型专注于相关信息并忽略不相关的干扰。

# 研究动机

- 当前的基准测试数据通常包含直接关联问题描述和解决方案的信息。这种理想化的数据集虽然便于评估模型性能，但并不总能反映现实世界中的问题复杂性。在实际情况中，问题描述通常包含大量信息，其中只有部分是解决问题所需的关键信息，而其他信息可能是无关的。因此有必要考虑解决问题途中哪些才是我们关注的信息。

- 心理学研究表明，无关信息的存在会干扰儿童及成人解决问题的能力。同样，LLM在处理信息时也可能受到无关信息的负面影响。因此，研究prompt中的上下文信息对模型性能的影响，并开发策略以增强模型对此类干扰的抵抗力，是提高其鲁棒性的关键。

  

# 实验内容

## 数据集

从GSM8K采样1000个问题，再从中选择LLM可以通过Prompt技术回答正确的100个问题作为简单集合，其中60%可以在两步推理内解决。

随后使用下图模板添加无关信息到句子里：（1）添加话题内和话题外的句子（2）填充角色名称（3）填充一些数字，进而得到最终的数据集GSM-IC，共包含58,052条数据。

![image-20240408101547150](assets\image-20240408101547150.png)

## 评估指标

微观准确率：平均回答问题准确度

宏观准确率：把问题分成n类，每一类中所有问题全对才算这类正确，随后计算平均类别准确度 

标准化准确率：某方法准确率/ Baseline准确率

## 评估模型和方法

模型：Code-davinci-002、text-davinci-003
方法：

- Chain-of-thought prompting (**Cot**)
- Zero-shot chain-of-thought prompting (**0-Cot**)
- Least-to-most prompting (**LTM**，**切分问题到子问题，随后用COT分而治之解决子问题**)
- Program prompts (**PROGRAM**，**将数学问题视为程序，执行模型生成的Python程序去解决问题**)
- Self-consistency (**SC，LLM采样生成多组答案，随后投票选择多数**） -> 解码策略, 可以和以上所有方法一起使用

## Prompt设计

![image-20240408103016561](assets\image-20240408103016561.png)

## 实验结果

- 增加干扰项之后, 大部分方法的问题解决率都低于30%, 相比之前的100基线上的问题来看，下降特别大。
- LTM对无关上下文信息的抗干扰能力最强, 显著干预其他所有的方法
- 选择带有干扰的演示示例可以减轻无关信息的干扰作用，在GSM-IC数据集上，使用带干扰因素的示例始终优于不带干扰示例的prompt。后续还验证了使用干扰项的样本并不会降低LLM在原始GSM8K数据的性能，因此**好的提示设计是可以实现准确率和鲁棒性兼顾**
- Self-consistency解码方法可以提高LLM对无关上下文信息的抗干扰能力. 但是从Norm指标来看，目前最佳的组合的准确率仅45%，因此仍然需要探索更好的算法来降低无关因子对LLM的影响程度

![image-20240408103322532](assets\image-20240408103322532.png)

角色语义重叠和数字范围的影响在Micro指标上影响较小, 但是Macro指标上影响比较明显；而无关信息的主题则相对影响非常大，在micro指标上有8～10个百分点, macro几乎呈倍数变化. 整体上LTM的稳定性最佳。

![image-20240408103607987](assets\image-20240408103607987.png)

LTM在多步解答的复杂推理任务中，对无关上下文信息相对CoT和Program不那么敏感

![image-20240408103801180](assets\image-20240408103801180.png)

增加干扰项的演示示例可以提高对无关信息的鲁棒性，同如果添加"Feel free to ignore irrelevant information given in the questions" 也能起到同样的效果

[instruct + no distraction sample prompt ≈ no instruct + distraction sample] 这两种方式效果基本对等，但在没有无关信息引入的问题描述场景下，还能进一步提高效果

![image-20240408104038988](assets\image-20240408104038988.png)

过于复杂的prompts可能会损伤LLM对无关信息的鲁棒性

![image-20240408103904655](assets\image-20240408103904655.png)

# 总结

本研究着重探讨了在问题描述中存在无关上下文信息时，如何运用prompt技术来减轻这些干扰对模型性能的影响。研究发现，有三种主要策略可以有效地实现这一目标：

首先，通过引入示例性的干扰项，可以模拟数据增强的效果，从而增强模型的抗干扰能力。然而，示例的数量需要仔细控制，过多的干扰项可能会适得其反，影响模型的效果。

其次，采用忽略无关信息的任务指令，让模型遵循人类的指导来完成任务。

最后，运用推理分解或自我一致性（SC）解码的方法，可以进一步提升LLM抵抗干扰的能力。