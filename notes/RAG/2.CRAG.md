



# **洞见**

大型语言模型 （LLMs） 不可避免地会出现幻觉，因为仅通过它们所封装的参数知识无法确保生成文本的准确性。尽管检索增强生成 （RAG） 是对LLM的可行补充，但它在很大程度上依赖于被检索的文档的相关性，这引发了人们对检索出错时模型行为的担忧。今天我们要同大家分享研究工作——CRAG，即矫正式检索增强生成，该框架旨在评估查询检索文档的整体质量来提高生成的鲁棒性，解决检索质量不高导致生成文本错误和幻觉的问题。CRAG是即插即用的，可以与各种基于RAG的方法无缝耦合。在涵盖短格式和长格式生成任务的四个数据集上的实验表明，CRAG可以显著提高基于RAG的方法的性能。

# **工作动机**

大型语言模型（LLM）因其出色的指令理解和流畅文本生成能力而受到广泛关注。但LLM常因事实错误而产生幻觉，无法仅靠内置知识确保文本准确。为解决这一问题，研究者引入了检索技术，如RAG，通过外部知识库中的文档增强模型输入。RAG虽有效补充了LLM，但依赖于检索文档的准确性，一旦检索失败或结果不准确，模型性能便受影响。如Figure 1所示，低质量的检索器（Retriever）容易引入大量不相关的信息，阻碍模型获取准确的知识并可能误导它们，从而导致幻觉等问题。

![image-20240320105844248](assets\image-20240320105844248.png)

然而，大多数传统的 RAG 方法不分青红皂白地合并检索到的文档，无论这些文档是否相关。此外，目前的方法大多将完整的文档视为检索和使用过程中的参考知识。但是，这些检索到的文档中的相当一部分文本对于生成来说往往是非必需的，这不应该在RAG中被平等地引用和参与。

论文针对检索结果不准确的情况，提出了Corrective Retrieval-Augmented Generation（CRAG）方法，旨在自我修正检索结果并提升文档在文本生成中的有效性。CRAG包含一个轻量级评估器，用于评估查询相关文档的质量，是RAG中审查检索文件相关性和可靠性的核心，根据评估结果触发相应的纠正操作。为扩展信息范围，CRAG结合了大规模网络搜索，弥补了静态语料库的局限。同时，通过分解重构算法，CRAG排除了无关上下文，确保了关键信息的精确提取，优化了知识利用。

# **方法架构**

![image-20240320112237598](assets\image-20240320112237598.png)

针对上述问题，论文专门研究了检索器返回不准确结果的场景。该文提出一种名为Corrective Retrieval-Augmented Generation（CRAG）的方法，对检索器的结果进行自我校正，提高文档对增强生成的利用率。**轻量级检索评估器**旨在评估查询检索文档的整体质量。这是RAG中的关键组成部分，通过审查和评估检索到的文件的相关性和可靠性，有助于生成信息。置信度是量化的，基于该置信度可以触发 【正确， 错误， 模糊】的不同知识检索操作。对于后两个动作，**大规模网络搜索被整合为一种战略扩展**，因为从静态和有限的语料库中检索只能返回范围和多样性方面的次优文档。这种增强是为了扩大检索信息的范围，利用网络的扩展性和动态性来补充和丰富最初获得的文档。此外，为了消除检索到的文档中包含的对 RAG 无益的冗余上下文，在整个检索和利用过程中精心设计了一种**分解然后重构算法**。该算法确保了检索到的信息的细化，**优化了关键见解的提取，并最大限度地减少了非必要元素**，从而提高了检索到的数据的利用率。

## 模型推理概述

![image-20240320112908991](assets\image-20240320112908991.png)

具体而言，给定一个输入查询和任意检索器检索的文档，构建一个轻量级的检索评估器来估计检索文档与输入查询的相关度分数。计算得到的相关度分数被量化为三个置信度，并触发相应的动作：{正确，错误，模糊}({Correct, Incorrect, Ambiguous})。如果触发了“正确”操作，则检索到的文档将被细化为更精确的知识条目。此优化操作涉及知识分解、筛选和重组。如果触发了“错误”操作，则检索到的文档将被丢弃。取而代之的是，网络搜索被诉诸并被视为更正的补充知识来源。最终，当系统无法自信地做出正确或错误的判断时，就会触发“模糊”，将网络搜索和知识提炼相结合。在优化检索结果后，可以采用任意生成模型来生成最终结果。

## 检索评估器

检索评估器的准确性对整体系统性能起着关键作用，论文的目标是在文档不相关时对其进行纠正。具体而言，采用并微调了T5-large。对于每个问题，通常检索到10个文档。问题与每个文档连接作为输入，评估器针对每个问题-文档对单独预测相关性分数。在微调过程中，正样本的标签为1，负样本的标签为-1。在推理时，评估器为每个文档打分-1到1的相关性。论文还尝试提示ChatGPT来识别检索相关性进行比较，但其表现不佳。

## 动作触发

基于前述每个检索文档的置信度分数，设定上下阈值后相应地设计和触发三种类型的动作。 如果置信度分数高于上阈值，则识别文档为“正确”，如果低于下阈值，则识别为“错误”。否则，执行“模糊”。这个过程针对每个检索文档单独进行。

**正确**：当至少有一个检索文档的置信度分数高于上阈值时，检索就是“正确”的。如果如此，意味着检索结果中存在相关文档。即使能找到相关文档，这份文档中也不可避免地存在一些噪声知识条。为了提取这份文档中最关键的知识条，设计了一个知识提炼方法（见后续小节）。

**错误**：如果所有检索文档的置信度分数都低于下阈值，则假设检索为“错误”。这表明所有检索文档都被认为不相关，对生成无益。这里引入网络搜索，从互联网进行搜索，这一纠正动作帮助克服了无法参考可靠知识的尴尬挑战。

**模糊**：除上述两种情况外，剩余的将分配给中间动作“模糊”。由于检索评估器对其判断不自信，“正确”和“错误”两种处理后的知识会结合以互补。实施这种策略可以显著增强系统的鲁棒性。

## 知识提炼

给定一个相关的检索文档，设计了一个分解-重组的知识提炼方法，以进一步提取其中最关键的知识条。首先，通过启发式规则将每个检索文档分割成细粒度的知识条，然后利用微调的检索评估器计算每个知识条的相关性得分。基于这些分数，过滤掉不相关的知识条，同时按顺序通过连接相关知识条进行重组，即内部知识。

## 网络搜索

如果假定检索结果全部不相关，那么寻求互补的外部知识就极为重要。由于从静态和有限的语料库中检索只能返回范围和多样性方面次优的文档，因此将大规模网络搜索集成为RAG的战略扩展。具体而言，利用ChatGPT将输入问题重写成由关键词组成的查询，以模拟搜索引擎的日常使用。

# **实验设置与结果**

## 数据集和评估指标

CRAG在四个数据集上进行了评估，包括:

- PopQA是一个短文本生成任务，包含1399个问题。通常，每个问题只期望回答一个事实知识实体。采用准确率作为评估指标。
- Biography是一个长文本生成任务，任务是生成关于某个实体的详细传记。遵循前面的工作，采用FactScore来评估生成的传记。
- PubHealth是一个医疗保健领域的任务，由真假问题组成。声明包含关于健康的事实信息，模型的任务是验证真伪并给出判断。采用准确率作为评估指标。
- Arc-Challenge是一个关于一些日常常识科学现象的多项选择题任务。给定日常生活中发生的一个科学事件，模型需要在3或4个可选选择中选择正确的描述。同样采用准确率作为评估指标。

## 主实验和结果

![image-20240320113748597](assets\image-20240320113748597.png)

结果如Table 1所示，将所提出的方法与标准RAG耦合的模型命名为CRAG，与Self-RAG耦合的模型命名为Self-CRAG。从这些结果中可以看出，所提方法可以显著提高RAG和Self-RAG的性能。

## 消融实验和结果

探索每个触发动作的影响。

![image-20240320202047448](assets\image-20240320202047448.png)

结果如Table 2所示，在PopQA数据集上进行评估，以展示准确率方面的性能变化。具体而言，当移除Correct或Incorrect动作时，它被合并到Ambiguous，这样原本触发Correct或Incorrect的比例会触发Ambiguous。另一方面，当移除Ambiguous动作时，只有一个阈值，所有输入查询明确触发Correct或Incorrect。从这些结果可以看出，无论移除哪个动作，性能都有下降，说明每个动作都有助于提高生成的鲁棒性。

探索每个知识利用操作的影响。

![image-20240320203242033](assets\image-20240320203242033.png)

Table 3说明如果移除关键知识利用操作，性能会发生何种变化。在PopQA数据集上针对准确率进行评估，通过单独移除文档提炼、搜索查询重写和外部知识选择等知识利用操作。移除文档提炼表示直接馈送原始检索文档给后续生成器、移除搜索查询重写表示在知识搜索期间不会将问题重写为由关键词组成的查询、移除知识选择表示不进行选择地将所有搜索到的网页内容都视为外部知识。可以发现无论移除哪个知识利用操作，最终系统的性能都有所下降，这表明每个知识利用操作都有助于改进知识的利用。

##   检索评估器的准确性

![image-20240320204346670](assets\image-20240320204346670.png)

检索评估器的质量在很大程度上决定了整个系统的性能。给定文档检索结果，本文评估检索评估器是否可以准确判断这些结果的整体质量。Table 4结果显示，轻量级基于T5的检索评估器在所有设置下都明显优于具有竞争力的ChatGPT。

## **对检索性能的鲁棒性**

![image-20240320205227834](assets\image-20240320205227834.png)

为进一步验证提出方法对检索性能的鲁棒性，论文研究了在不同检索性能下生成性能的变化。随机移除部分准确的检索结果来模拟低质量检索器，并评估性能变化。Figure 3展示了Self-RAG和Self-CRAG在PopQA数据集上的性能变化。可以看出，随着检索性能的下降，Self-RAG和Self-CRAG的生成性能也下降，表明生成器高度依赖于检索器的质量。此外，随着检索性能的下降，Self-CRAG的生成性能下降得比Self-RAG轻微。这些结果暗示在增强对检索性能的鲁棒性方面，Self-CRAG优于Self-RAG。

# Takeaway

本研究深入探讨了检索器在提供不精确结果时的应对策略，并首次尝试针对RAG开发出一套创新的纠正机制，旨在增强其鲁棒性。论文中提出了一种新颖的即插即用型方法——CRAG，该方法专注于提升模型的自动自我修正能力，并更高效地利用检索到的文档资源。通过对一系列实验的细致分析，我们发现CRAG方法不仅在各种基于RAG的模型上展现出了良好的适应性，而且在处理短文本与长文本生成任务时，均表现出了卓越的泛化性能。这一发现为未来在自然语言处理领域的研究和应用提供了宝贵的参考，特别是在提高生成模型准确性和可靠性方面。
